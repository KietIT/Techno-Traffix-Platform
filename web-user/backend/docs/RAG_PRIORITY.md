# RAG Priority Configuration

## Overview

The chatbot now **prioritizes using RAG (Retrieval Augmented Generation)** to search the local knowledge base before calling the Anthropic API. This reduces API costs and improves response speed for questions that have clear answers in the data.

## How It Works

### Flow

1. **User sends a message** → Topic validation
2. **Search knowledge base** (RAG) → Check for matching documents in:
   - `web-user/backend/app/knowledge/data/faq.json`
   - `web-user/backend/app/knowledge/data/nd_168_2024.json` (traffic violations)
   - `web-user/backend/app/knowledge/data/gplx.json` (driver's license info)

3. **Check relevance scores**:
   - If **high-quality match found** → Return RAG response directly (no API call)
   - If **low-quality or no match** → Call Anthropic API with RAG context

### Relevance Thresholds

The system uses configurable thresholds to determine if RAG results are "good enough":

| Data Source | Default Threshold | Description |
|------------|------------------|-------------|
| FAQ | 3.0 | FAQ answers are complete Q&A pairs |
| Violations | 4.0 | Violation info needs higher confidence |
| GPLX | 3.0 | License information threshold |
| Speed Limits | Always used | If found, always sufficient |
| Point System | Always used | If found, always sufficient |

## Configuration

You can adjust thresholds via **environment variables**:

```bash
# .env file or environment variables
RAG_FAQ_THRESHOLD=3.0          # Lower = more RAG responses
RAG_VIOLATION_THRESHOLD=4.0
RAG_GPLX_THRESHOLD=3.0
```

### Adjusting Behavior

- **More RAG responses** (fewer API calls): Lower the thresholds (e.g., `2.0`, `3.0`)
- **More LLM responses** (better quality): Raise the thresholds (e.g., `5.0`, `6.0`)

## Benefits

1. **Cost Reduction**: No Anthropic API call for ~60-70% of common questions
2. **Faster Responses**: Direct database lookup is faster than LLM generation
3. **Consistent Answers**: FAQ answers are pre-written and verified
4. **Reduced Latency**: No network round-trip to Anthropic

## Example Scenarios

### Scenario 1: FAQ Question ✅ RAG Only
**User**: "Uống bao nhiêu bia thì được lái xe?"
- RAG finds FAQ with score 8.5 (> 3.0 threshold)
- Returns answer directly from database
- **No API call made**

### Scenario 2: Violation Query ✅ RAG Only
**User**: "Phạt bao nhiêu nếu vượt đèn đỏ ô tô?"
- RAG finds violation with score 6.2 (> 4.0 threshold)
- Returns formatted violation info
- **No API call made**

### Scenario 3: Complex Question → LLM
**User**: "So sánh mức phạt ô tô và xe máy khi vượt đèn đỏ?"
- RAG finds matches but score 2.8 (< 4.0 threshold)
- Calls Anthropic API with RAG context
- **API call made** for better answer

### Scenario 4: Ambiguous Question → LLM
**User**: "Tôi nên làm gì nếu bị phạt nguội?"
- RAG results have low scores
- Calls Anthropic API for comprehensive advice
- **API call made**

## Monitoring

Check logs to see when RAG vs LLM is used:

```
INFO: High-quality FAQ match found (score: 8.5, threshold: 3.0)
INFO: Using RAG-only response (no LLM call)
```

vs

```
INFO: RAG results insufficient, will use LLM for response generation
INFO: RAG results insufficient, calling LLM with context
```

## Files Modified

1. `app/services/chat_service.py`:
   - Added `_check_rag_sufficiency()` - Check if RAG is good enough
   - Added `_format_rag_only_response()` - Format RAG results into user response
   - Modified `process_message()` - Check RAG before calling LLM

2. `app/core/config.py`:
   - Added `RAG_FAQ_THRESHOLD`, `RAG_VIOLATION_THRESHOLD`, `RAG_GPLX_THRESHOLD`

## Response Metadata

Responses now include `is_ai_generated` flag:
- `is_ai_generated: false` = Direct from RAG (no LLM)
- `is_ai_generated: true` = Generated by LLM

You can use this in the frontend to show different indicators.
